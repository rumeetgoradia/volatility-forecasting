data:
  raw_path: "data/raw/futures_data.parquet"
  processed_path: "data/processed/"
  features_path: "data/features/"
  regimes_path: "data/regimes/"

  instruments:
    - "INDX.CAC"
    - "INDX.SPX"
    - "INDX.TPX"
    - "INDX.UKX"
    - "INDX.NDX"
    - "INDX.FTSEMIB"
    - "INDX.SMI"

  split:
    train_start: "2000-01-01"
    train_end: "2018-12-31"
    val_start: "2019-01-01"
    val_end: "2021-12-31"
    test_start: "2022-01-01"
    test_end: "2025-04-30"

preprocessing:
  remove_zero_volume: true
  outlier_threshold: 0.05
  min_records_per_day: 50

features:
  rv:
    freq: "5T"
    aggregation_periods: ["1D", "1W", "1M"]

  lags:
    daily: [1, 5, 22]
    weekly: [1, 4]
    monthly: [1, 3]

  time_features:
    - "hour"
    - "day_of_week"
    - "month"

  volume_features:
    - "volume"
    - "volume_ma_20"

models:
  har_rv:
    horizons: [1, 5, 22]

  lstm:
    hidden_size: 64
    num_layers: 2
    dropout: 0.2
    sequence_length: 20
    batch_size: 256
    learning_rate: 0.001
    epochs: 50
    patience: 10

  tcn:
    num_channels: [32, 64, 128]
    kernel_size: 3
    dropout: 0.2
    sequence_length: 20
    batch_size: 256
    learning_rate: 0.001
    epochs: 50
    patience: 10

  chronos:
    model_name: "amazon/chronos-bolt-small"
    device: "cuda"
    prediction_length: 1
    num_samples: 20         # Lower this to 10 if you hit Out-of-Memory (OOM)
    context_length: 64

regimes:
  method: "hmm"
  n_regimes: 3
  features:
    - "RV_1D"
    - "volume_log"

  hmm:
    n_components: 3
    covariance_type: "full"
    n_iter: 100
    random_state: 42

  kmeans:
    n_clusters: 3
    n_init: 10
    random_state: 42

moe:
  experts:
    - "har_rv"
    - "lstm"
    - "tcn"
    - "chronos"

  gating:
    hidden_size: 32
    num_layers: 2
    dropout: 0.1
    use_regime_supervision: true

  training:
    freeze_experts: true
    joint_finetune: false
    batch_size: 256
    learning_rate: 0.001
    epochs: 30
    patience: 10

random_seed: 42